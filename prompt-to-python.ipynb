{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer1 = AutoTokenizer.from_pretrained(\"suriya7/Gemma-2B-Finetuned-Python-Model\")\nmodel1 = AutoModelForCausalLM.from_pretrained(\"suriya7/Gemma-2B-Finetuned-Python-Model\")\n\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = input('enter a query:')\nprompt_template = f\"\"\"\n<start_of_turn>user based on given instruction create a solution\\n\\nhere are the instruction {query}\n<end_of_turn>\\n<start_of_turn>model\n\"\"\"\nprompt = prompt_template\nencodeds = tokenizer1(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel1.to(device)\ninputs = encodeds.to(device)\n\n\n# Increase max_new_tokens if needed\ngenerated_ids = model1.generate(inputs, max_new_tokens=1000, do_sample=False, pad_token_id=tokenizer1.eos_token_id)\nans = ''\nfor i in tokenizer1.decode(generated_ids[0], skip_special_tokens=True).split('<end_of_turn>')[:2]:\n    ans += i\n\n# Extract only the model's answer\nmodel_answer = ans.split(\"model\")[1].strip()\nprint(model_answer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes==0.42.0\n!pip install -q -U peft==0.8.2\n!pip install -q -U trl==0.7.10\n!pip install -q -U accelerate==0.27.1\n!pip install -q -U transformers==4.38.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q -U datasets==2.16.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = \"suriya7/Gemma-2B-Finetuned-Python-Model\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"flytech/python-codes-25k\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:52:48.857084Z","iopub.execute_input":"2024-03-20T10:52:48.857488Z","iopub.status.idle":"2024-03-20T10:52:55.080877Z","shell.execute_reply.started":"2024-03-20T10:52:48.857458Z","shell.execute_reply":"2024-03-20T10:52:55.079638Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['output', 'instruction', 'input', 'text'],\n        num_rows: 49626\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"train\"][:20][\"instruction\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:52:55.082795Z","iopub.execute_input":"2024-03-20T10:52:55.083374Z","iopub.status.idle":"2024-03-20T10:52:55.091665Z","shell.execute_reply.started":"2024-03-20T10:52:55.083345Z","shell.execute_reply":"2024-03-20T10:52:55.090487Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['Help me set up my daily to-do list!',\n 'Create a shopping list based on my inputs!',\n 'Calculate how much time I spend on my phone per week!',\n 'Help me split the bill among my friends!',\n 'Organize my movie list into genres!',\n 'Calculate the average rating of my book collection!',\n 'Create a playlist based on my mood!',\n 'Help me find the best deals on my shopping list!',\n 'Calculate how much I need to save per month for my vacation!',\n 'Determine the most efficient route for my errands!',\n 'Help me manage my subscriptions!',\n 'Create a meal plan for the week!',\n 'Calculate my carbon footprint based on my daily activities!',\n 'Help me set reminders for my favorite TV shows!',\n 'Assist me in managing my study schedule!',\n 'Help me calculate the ROI for my investments!',\n 'Assist me in tracking my water intake!',\n 'Help me find the best time to post on social media based on my followers!',\n 'Assist me in calculating my monthly budget!',\n 'Calculate my coffee consumption']"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"train\"][0][\"output\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"Generate input text based on a prompt, task instruction, (context info), and answer.\n\n    :param data_point: dict: Data point\n    :return: dict: Data point with the added \"prompt\" field\n    \"\"\"\n    prefix_text = 'based on given instruction create a solution\\n\\n'\n    prompt_text = f\"\"\"<start_of_turn>user {prefix_text} here are the instruction {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model {data_point[\"output\"]} <end_of_turn>\"\"\"\n    data_point[\"prompt\"] = prompt_text\n    return data_point\n\n# Add the \"prompt\" column to the dataset\ndataset = dataset['train'].map(generate_prompt)\n\n# Print the updated dataset\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset[0][\"prompt\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modules = find_all_linear_names(model)\nprint(modules)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,  # Adjust batch size based on GPU memory\n        gradient_accumulation_steps=5,\n        warmup_steps=100,  # Adjust warm-up steps\n        num_train_epochs=1,  # Adjust the number of training epochs\n        max_steps=500,  # Adjust the maximum number of steps\n        learning_rate=1e-4,  # Adjust the learning rate\n        logging_steps=10,  # Adjust how often training information is logged\n        output_dir=\"python_model\",  # Change the output directory\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = \"gemma-finetuned-python_code\"\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model_new\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model_new_tokenizer\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmerged_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/merged_model_new\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/merged_model_new_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:50:55.626796Z","iopub.execute_input":"2024-03-20T10:50:55.627208Z","iopub.status.idle":"2024-03-20T10:51:10.277993Z","shell.execute_reply.started":"2024-03-20T10:50:55.627179Z","shell.execute_reply":"2024-03-20T10:51:10.276859Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10f14828bfd4d4b8827dc13fb6dc031"}},"metadata":{}}]},{"cell_type":"code","source":"query = input('enter a query:')\nprompt_template = f\"\"\"\n<start_of_turn>user based on given instruction create a solution\\n\\nhere are the instruction {query}\n<end_of_turn>\\n<start_of_turn>model\n\"\"\"\nprompt = prompt_template\nencodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmerged_model.to(device)\ninputs = encodeds.to(device)\n\n\n# Increase max_new_tokens if needed\ngenerated_ids = merged_model.generate(inputs, max_new_tokens=1000, do_sample=False, pad_token_id=tokenizer.eos_token_id)\nans = ''\nfor i in tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('<end_of_turn>')[:2]:\n    ans += i\n\n# Extract only the model's answer\nmodel_answer = ans.split(\"model\")[1].strip()\nprint(model_answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:14:41.922050Z","iopub.execute_input":"2024-03-20T11:14:41.923040Z","iopub.status.idle":"2024-03-20T11:15:20.568639Z","shell.execute_reply.started":"2024-03-20T11:14:41.923002Z","shell.execute_reply":"2024-03-20T11:15:20.567617Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdin","text":"enter a query: Generate a code to implement the quicksort algorithm in Python\n"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Python code to implement the quicksort algorithm:\n\n```python\ndef quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    else:\n        pivot = arr[len(arr) // 2]\n        less = [x for x in arr if x < pivot]\n        equal = [x for x in arr if x == pivot]\n        greater = [x for x in arr if x > pivot]\n        return quicksort(less) + equal + quicksort(greater)\n```\nThis code uses the divide and conquer approach to sort an array. It first divides the array into two halves, then recursively sorts the two halves and finally merges them together.\n```\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset[\"train\"][49007][\"output\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:15:20.570178Z","iopub.execute_input":"2024-03-20T11:15:20.570492Z","iopub.status.idle":"2024-03-20T11:15:20.576240Z","shell.execute_reply.started":"2024-03-20T11:15:20.570465Z","shell.execute_reply":"2024-03-20T11:15:20.575266Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\"\"\"\nImplement quicksort algorithm\n\"\"\"\ndef partition(arr,low,high): \n    i = ( low-1 )          \n    pivot = arr[high]     \n    for j in range(low , high): \n        if arr[j] <= pivot:             \n            i = i+1 \n            arr[i],arr[j] = arr[j],arr[i] \n    arr[i+1],arr[high] = arr[high],arr[i+1] \n    return ( i+1 ) \n  \ndef quick_sort(arr,low,high): \n    if low < high: \n        pi = partition(arr,low,high) \n        quick_sort(arr, low, pi-1) \n        quick_sort(arr, pi+1, high) \n\nif __name__ == '__main__':\n    arr = [9,4,2,7,3,8,5,6]\n    n = len(arr) \n    quick_sort(arr,0,n-1)\n    print(\"The sorted array is:\")\n    for i in arr: \n        print (i),\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[\"train\"][49000:49010][\"instruction\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:03:33.947142Z","iopub.execute_input":"2024-03-20T11:03:33.947545Z","iopub.status.idle":"2024-03-20T11:03:33.954660Z","shell.execute_reply.started":"2024-03-20T11:03:33.947515Z","shell.execute_reply":"2024-03-20T11:03:33.953701Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['Consider a way to detect malicious HTTP requests using Python',\n 'Create a Python program to print out the lyrics of the song \"Happy Birthday\"',\n 'Create a web-scraping program to extract data from twitter',\n 'Create a Python program to convert a list of values from Celsius to Fahrenheit [0, 10, 20]',\n 'Create an output array from the given input so that each element in the output is the product of all the other element in the same row [\\n [10, 2, 3],\\n [4, 5, 6],\\n [7, 8, 9]\\n]',\n 'Generate a list comprehension in Python that prints the cubes of the first 5 numbers',\n 'Write a python script that computes the sum of all the odd numbers between 1 and 1000',\n 'Generate a code to implement the quicksort algorithm in Python',\n 'Write a python program to transform input set of numbers into a list of tuples 1, 2, 3, 4, 5',\n 'Create a Python program to scrape and store the HTML content of a given website']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}